#
# Be aware that even a small syntax error here can lead to failures in output.
#

sidebar:
    position: left # position of the sidebar : left or right
    about: False # set to False or comment line if you want to remove the "how to use?" in the sidebar
    education: True # set to False if you want education in main section instead of in sidebar

    # Profile information
    name: Mike Wilkins
    tagline: HPC/AI Researcher
    avatar: headshot.webp  #place a 100x100 picture inside /assets/images/ folder and provide the name of the file below

    # Sidebar links
    email: wilkins@anl.gov
    # timezone: America/Cancun Timezone
    # citizenship:
    # website: blog.webjeda.com #do not add http://
    linkedin: mjwilkins418
    # xing: alandoe
    github: mjwilkins18
    # telegram: # add your nickname without '@' sign
    # gitlab:
    # bitbucket:
    # twitter: '@webjeda'
    # stack-overflow: # Number/Username, e.g. 123456/alandoe
    # codewars:
    # goodreads: # Number-Username, e.g. 123456-alandoe
    # pdf: http://www.africau.edu/images/default/sample.pdf

    # languages:
    #  title: Languages
    #  info:
    #    - idiom: English
    #      level: Native
    #
    #    - idiom: French
    #      level: Professional
    #
    #    - idiom: Spanish
    #      level: Professional

    # interests:
    #  title: Interests
    #  info:
    #    - item: Climbing
    #      link:
    #
    #    - item: Snowboarding
    #      link:
    #
    #    - item: Cooking
    #      link:

career-profile:
    title: Welcome!
    summary: |
        My name is Mike Wilkins, and I research optimizations for AI workloads on high-performance computing systems.
        As a Maria Goeppert Mayer Fellow at Argonne National Laboratory, I'm currently leading the development of a new holistic online autotuner. 
        I previously completed my Ph.D. in Computer Engineering at Northwestern University and have industry experience at Cornelis Networks and Meta.
        I am open to collaboration opportunities, please feel free to reach out with ideas or questions!

education:
    title: Education
    info:
      - degree: Ph.D. Computer Engineering
        university: Northwestern University
        time: 2023
      - degree: M.S. Computer Engineering
        university: Northwestern University
        time: 2021
      - degree: B.S. Computer Engineering
        university: Rose-Hulman Institute of Technology
        time: 2019

experiences:
    title: Experiences
    info:
      - role: Maria Goeppert Mayer Fellow
        time: Oct 2024 - Present
        company: Argonne National Laboratory
        details: |
            - Directed a $1M+ lab-funded research project, "Holistic Machine Learning Autotuning for Massive-Scale Artificial Intelligence for Science," supervising a team of 3 students to advance AI-driven scientific computing
            - Developed and deployed ACCLAiM, an autotuner for collective communication, on production supercomputers including Aurora, delivering widespread performance improvements with speedups of up to 35x
            - Contributed major enhancements to MPICH, the leading open-source MPI implementation, with a focus on optimizing collective communication for high-performance computing environments.
      - role: Software Engineer
        time: Jan-Sep 2024
        company: Cornelis Networks
        details: |
            - Optimized the OPX libfabric provider, achieved a 5x bandwidth improvement for GPU communication among other advancements
            - Led the development of the reference libfabric provider for the Ultra Ethernet Consortium
            - Created developer productivity tooling, including an OPX performance profiler and a runtime parameter autotuner

      - role: AI Research Intern
        time: Summer 2023
        company: Meta
        details: |
            - Designed and implemented an application-aware communication (NCCL) autotuner for large-scale AI workloads
            - Developed an AI application emulation tool that mimics production models by overlapping communication and genericized compute kernels

      - role: Research Aide/Visiting Student
        time: 2020 - 2023
        company: Argonne National Laboratory
        details: |
            - Founded the MPI collective algorithm/machine learning project, initially under the supervision of Dr. Min Si and Dr. Pavan Balaji, later Dr. Yanfei Guo and Dr. Rajeev Thakur
            - Earned perpetual external funding from ANL for the remainder of my Ph.D

      - role: Engineering Leadership Program Intern
        time: Summer 2018
        company: National Instruments
        details: |
            - Engaged with technical leaders through field presentations to multiple companies in the Seattle area
            - Assisted customers to design and troubleshoot data-acquisition applications using NI platforms

      - role: Trailblazer Intern
        time: Summer 2017
        company: Flexware Innovation
        details: |
            - Designed an innovative RFID tracking solution to repair a malfunctioning inventory locating system
            - Produced a full-stack BI database solution analyzing internal employee and revenue data

      - role: Director of Tool Services
        time: Summer 2016
        company: Power Solutions International
        details: |
            - Organized and managed the companyâ€™s inventory of CNC machining tools, valued at more than $500,000
            - Trained company technicians on new processes and managed tool services employees

projects:
    title: Sample Research Projects
    intro: >
          Here is a high-level description of some of my active and former research projects.
    info:
      - role: Holistic Online Autotuning for Large-Scale Artificial Intelligence
        time: Ongoing
        details: |
            - Creating a new autotuner for long-running distributed AI training workloads that improves model quality and workload efficiency during execution

      - role: ML Autotuning for Generalized MPI Collective Algorithms
        time: 2021-2024
        details: |
            - Created new MPI collective algorithms and a machine-learning autotuner (ACCLAiM) that automatically selects and optimizes the best algorithm
            - Invented multiple optimizations to make ML-based MPI autotuning feasible on large-scale systems
     
      - role: High-Level Parallel Languages for HPC
        time: 2019-2023
        details: |
            - Developing a new hardware/software co-design for the Standard ML language targeted at HPC systems and applications, including AI
            - Created a new version of the NAS benchmark suite using MPL (a parallel compiler for Standard ML) to enable direct comparison between HLPLs and lower-level languages for HPC

      - role: Cache Coherence for High-Level Parallel Languages
        time: 2019-2022
        details: |
            - Identified a low-level memory property called WARD that can be introduced by construction in high-level parallel programs
            - Implemented a custom cache coherence protocol in the Sniper architectural simulator and found an average speedup of 1.46x across the PBBS benchmark suite.

publications:
    title: Publications
    papers:
      - title: "On Transparent Optimizations for Communication in Highly Parallel Systems"
        link: "assets/pdfs/thesis-tech-report.pdf"
        authors: <strong>Michael Wilkins</strong>
        conference: Ph.D. Thesis

      - title: "Generalized Collective Algorithms for the Exascale Era"
        link: "assets/pdfs/gen-collectives.pdf"
        authors: <strong>Michael Wilkins</strong>, Hanming Wang, Peizhi Liu, Bangyen Pham, Yanfei Guo, Rajeev Thakur, Nikos Hardavellas, and Peter Dinda
        conference: CLUSTER'23

      - title: "Evaluating Functional Memory-Managed Parallel Languages for HPC using the NAS Parallel Benchmarks"
        link: "assets/pdfs/nas-mpl.pdf"
        authors: <strong>Michael Wilkins</strong>, Garrett Weil, Luke Arnold, Nikos Hardavellas, Peter Dinda      
        conference: HIPS'23 Workshop

      - title: "WARDen: Specializing Cache Coherence for High-Level Parallel Languages"
        link: "assets/pdfs/warden.pdf"
        authors: <strong>Michael Wilkins</strong>, Sam Westrick, Vijay Kandiah, Alex Bernat, Brian Suchy, Enrico Armenio Deiana, Simone Campanoni, Umut Acar, Peter Dinda, Nikos Hardavellas
        conference: CGO'23

      - title: "Program State Element Characterization"
        link: "assets/pdfs/psec.pdf"
        authors: Enrico Deiana, Brian Suchy, <strong>Michael Wilkins</strong>, Brian Homerding, Tommy McMichen, Katarzyna Dunajewski, Nikos Hardavellas, Peter Dinda, Simone Campanoni
        conference: CGO'23
     
      - title: "ACCLAiM: Advancing the Practicality of MPI Collective Communication Autotuning Using Machine Learning"
        link: "assets/pdfs/acclaim.pdf"
        authors: <strong>Michael Wilkins</strong>, Yanfei Guo, Rajeev Thakur, Peter Dinda, Nikos Hardavellas      
        conference: CLUSTER'22

      - title: "A FACT-Based Approach: Making Machine Learning Collective Autotuning Feasible on Exascale Systems"
        link: "assets/pdfs/fact.pdf"
        authors: <strong>Michael Wilkins</strong>, Yanfei Guo, Rajeev Thakur, Nikos Hardavellas, Peter Dinda, Min Si      
        conference: ExaMPI'21 Workshop

 

skills:
    title: Skills

    toolset:
      - name: Software/Scripting Languages
      - list: C, C++, Python, Standard/Parallel ML, C#, LabVIEW, Java, SQL, Bash
      - name: Parallel Programming/Communication
      - list: MPI, Libfabric, NCCL, CUDA, PyTorch, Parallel ML
      - name: Simulators/Tools
      - list: Sniper, gem5, ZSim, Xilinx Vivado, Xilinx ISE, Quartus II
      - name: Hardware Description Languages
      - list: Chisel, VHDL, Verilog, SPICE

footer: >
    Credit to <a href="http://themes.3rdwavemedia.com" target="_blank" rel="nofollow">Xiaoying Riley</a> for the base webpage template

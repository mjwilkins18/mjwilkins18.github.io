# # Be aware that even a small syntax error here can lead to failures in output.
sidebar:
  position: left
  # position of the sidebar : left or right
  about: False
  # set to False or comment line if you want to remove the "how to use?" in the sidebar
  education: True
  # set to False if you want education in main section instead of in sidebar
  # Profile information
  name: Mike Wilkins
  tagline: HPC/AI Researcher
  avatar: headshot.webp
  #place a 100x100 picture inside /assets/images/ folder and provide the name of the file below
  # Sidebar links
  email: wilkins@anl.gov
  linkedin: mjwilkins418
  github: mjwilkins18
career-profile:
  title: Welcome!
  summary: |
    My name is Mike Wilkins, and I research optimizations for AI workloads on high-performance computing systems. As a Maria Goeppert Mayer Fellow at Argonne National Laboratory, I'm currently leading the development of a new holistic online autotuner. I previously completed my Ph.D. in Computer Engineering at Northwestern University and have industry experience at Cornelis Networks and Meta. I am open to collaboration opportunities, please feel free to reach out with ideas or questions!
    
education:
  title: Education
  info:
    - degree: Ph.D. Computer Engineering
      university: Northwestern University
      time: 2023
    - degree: M.S. Computer Engineering
      university: Northwestern University
      time: 2021
    - degree: B.S. Computer Engineering
      university: Rose-Hulman Institute of Technology
      time: 2019
experiences:
  title: Experiences
  info:
    - role: Maria Goeppert Mayer Fellow
      time: Oct 2024 - Present
      company: Argonne National Laboratory
      details: |
        - Directed an independent research program on autotuning and collective communication, supported by a 3-year, $1M award from Argonne
        - Translated my MPI autotuning research into production, achieving speedups up to 35x for collective operations on Argonne’s exascale system, Aurora
        - Contributed major enhancements to MPICH, the leading open-source MPI implementation, with a focus on optimizing collective communication for high-performance computing environments
    - role: Software Engineer
      time: Jan-Sep 2024
      company: Cornelis Networks
      details: |
        - Spearheaded major performance optimizations for the OPX libfabric provider, achieving 5× bandwidth improvements for GPU communications and other critical improvements
        - Led the architecture and development of the reference libfabric provider for the Ultra Ethernet Consortium, achieving a key milestone in the standard’s development
        - Created OPX developer tools, including a profiler and autotuner, boosting team velocity
    - role: AI Research Intern
      time: Summer 2023
      company: Meta
      details: |
        - Designed and implemented an application-aware communication (NCCL) autotuner for large-scale AI workloads
        - Developed an AI application emulation tool that mimics production models by overlapping communication and genericized compute kernels
    - role: Research Aide/Visiting Student
      time: 2020 - 2023
      company: Argonne National Laboratory
      details: |
        - Founded the MPI collective algorithm/machine learning project, initially under the supervision of Dr. Min Si and Dr. Pavan Balaji, later Dr. Yanfei Guo and Dr. Rajeev Thakur
        - Earned perpetual external funding from ANL for the remainder of my Ph.D
    - role: Undergraduate Internships
      company: Power Solutions International (2016), Flexware Innovation (2017), National Instruments (2018)
projects:
  title: Sample Research Projects
  intro: >
    Here is a high-level description of some of my active and former research projects.
  info:
    - role: ML Autotuning for MPI
      time: Ongoing
      details: |
        - Invented many optimizations to make ML-based MPI autotuning feasible on large-scale systems
        - Developed the world’s first exascale-capable MPI collective algorithm autotuner and achieved up to 20% speedups for production applications
        - Exploring new “holistic” tuning methodologies to encompass performance-critical parameters across the software stack, targeting large scale AI workloads
    - role: Algorithms for Collective Communication
      time: Ongoing
      details: |
        - Created new generalized MPI collective algorithms that expose a tunable radix and outperform the previous best algorithms by up to 4.5x
        - Exploring new generalized algorithms for GPU-specific collective communication (e.g., NCCL) and new abstractions (e.g., circulant graphs)
    - role: High-Level Parallel Languages for HPC
      time: 2019-2023
      details: |
        - Developed a new hardware/software co-design for the Standard ML language targeted at HPC systems and applications, including AI
        - Created a new version of the NAS benchmark suite using MPL (a parallel compiler for Standard ML) to enable direct comparison between HLPLs and lower-level languages for HPC
    - role: Cache Coherence for High-Level Parallel Languages
      time: 2019-2022
      details: |
        - Identified a low-level memory property called WARD in high-level parallel programs
        - Implemented a custom cache coherence protocol in the Sniper architectural simulator and found an average speedup of 1.46x across the PBBS benchmark suite.
publications:
  title: Publications
  papers:
    - title: "On Transparent Optimizations for Communication in Highly Parallel Systems"
      link: "assets/pdfs/thesis-tech-report.pdf"
      authors: <strong>Michael Wilkins</strong>
      conference: Ph.D. Thesis
    - title: "Generalized Collective Algorithms for the Exascale Era"
      link: "assets/pdfs/gen-collectives.pdf"
      authors: <strong>Michael Wilkins</strong>, Hanming Wang, Peizhi Liu, Bangyen Pham, Yanfei Guo, Rajeev Thakur, Nikos Hardavellas, and Peter Dinda
      conference: CLUSTER'23
    - title: "Evaluating Functional Memory-Managed Parallel Languages for HPC using the NAS Parallel Benchmarks"
      link: "assets/pdfs/nas-mpl.pdf"
      authors: <strong>Michael Wilkins</strong>, Garrett Weil, Luke Arnold, Nikos Hardavellas, Peter Dinda
      conference: HIPS'23 Workshop
    - title: "WARDen: Specializing Cache Coherence for High-Level Parallel Languages"
      link: "assets/pdfs/warden.pdf"
      authors: <strong>Michael Wilkins</strong>, Sam Westrick, Vijay Kandiah, Alex Bernat, Brian Suchy, Enrico Armenio Deiana, Simone Campanoni, Umut Acar, Peter Dinda, Nikos Hardavellas
      conference: CGO'23
    - title: "Program State Element Characterization"
      link: "assets/pdfs/psec.pdf"
      authors: Enrico Deiana, Brian Suchy, <strong>Michael Wilkins</strong>, Brian Homerding, Tommy McMichen, Katarzyna Dunajewski, Nikos Hardavellas, Peter Dinda, Simone Campanoni
      conference: CGO'23
    - title: "ACCLAiM: Advancing the Practicality of MPI Collective Communication Autotuning Using Machine Learning"
      link: "assets/pdfs/acclaim.pdf"
      authors: <strong>Michael Wilkins</strong>, Yanfei Guo, Rajeev Thakur, Peter Dinda, Nikos Hardavellas
      conference: CLUSTER'22
    - title: "A FACT-Based Approach: Making Machine Learning Collective Autotuning Feasible on Exascale Systems"
      link: "assets/pdfs/fact.pdf"
      authors: <strong>Michael Wilkins</strong>, Yanfei Guo, Rajeev Thakur, Nikos Hardavellas, Peter Dinda, Min Si
      conference: ExaMPI'21 Workshop
skills:
  title: Skills
  toolset:
    - name: Software/Scripting Languages
    - list: C, C++, Python, Standard/Parallel ML, C#, LabVIEW, Java, SQL, Bash
    - name: Parallel Programming/Communication
    - list: MPI, Libfabric, NCCL, CUDA, PyTorch, Parallel ML
    - name: Simulators/Tools
    - list: Sniper, gem5, ZSim, Xilinx Vivado, Xilinx ISE, Quartus II
    - name: Hardware Description Languages
    - list: Chisel, VHDL, Verilog, SPICE
footer: >
  Credit to <a href="http://themes.3rdwavemedia.com" target="_blank" rel="nofollow">Xiaoying Riley</a> for the base webpage template
